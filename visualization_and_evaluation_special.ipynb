{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Self-DACE Results for specific pics\n",
        "\n",
        "Name: Yunpei Gu (Team: Richard Zhao, Oliver Fritsche, Yunpei Gu)\n",
        "\n",
        "Class: CS 7180 Advanced Perception\n",
        "\n",
        "Date: 2025-09-22\n",
        "\n",
        "Purpose: Visualization and evaluation for Self-DACE low-light enhancement model on pics with non-white luminance, high contrast, and multi luminant source.\n",
        "\n",
        "Image selection:\n",
        "- Pic 1 & 2 — High-contrast scenes.\n",
        "- Pic 3 & 4 — Multiple illuminant sources.\n",
        "- Pic 5 & 6 — Single non-white illuminant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Model loaded from epoch_118_model.pth\n"
          ]
        }
      ],
      "source": [
        "# === Config & model load ===\n",
        "import os, glob, re\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Use a non-interactive backend when running without GUI; comment it out if you want interactive figures locally.\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from model import light_net\n",
        "from evaluate import calculate_metrics  # Use the project's metric functions\n",
        "\n",
        "# Paths (minimal changes) — point to the numeric mirror folders only\n",
        "LOW_DIR  = \"data/random/low_num\"\n",
        "HIGH_DIR = \"data/random/high_num\"   # Can be an empty folder\n",
        "MODEL_PTH = \"epoch_118_model.pth\"   # Ensure this file exists at the repo root\n",
        "RESIZE = (512, 512)\n",
        "ALPHA_SCALE = 0.9\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Load model\n",
        "model = light_net().to(DEVICE)\n",
        "ckpt = torch.load(MODEL_PTH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt)\n",
        "model.eval()\n",
        "print(\"Model loaded from\", MODEL_PTH)\n",
        "\n",
        "# Output folders\n",
        "os.makedirs(\"outputs/random/triptychs_num\", exist_ok=True)\n",
        "os.makedirs(\"outputs/random/iterations\",   exist_ok=True)\n",
        "os.makedirs(\"outputs/random/composites\",   exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Helper functions (no torchvision dependency) ===\n",
        "EXTS = [\".jpg\", \".png\", \".jpeg\", \".JPG\", \".PNG\", \".JPEG\"]\n",
        "\n",
        "def find_file_by_index(folder, idx):\n",
        "    for ext in EXTS:\n",
        "        p = os.path.join(folder, f\"{idx}{ext}\")\n",
        "        if os.path.exists(p):\n",
        "            return p\n",
        "    return None\n",
        "\n",
        "def load_image_as_tensor(path, size=RESIZE, device=DEVICE):\n",
        "    pil = Image.open(path).convert(\"RGB\").resize(size)\n",
        "    arr = np.array(pil).astype(np.float32) / 255.0\n",
        "    ten = torch.from_numpy(arr).permute(2,0,1).unsqueeze(0).to(device)\n",
        "    return pil, ten\n",
        "\n",
        "def tensor_to_uint8_img(t):\n",
        "    if t.ndim == 4:\n",
        "        t = t.squeeze(0)\n",
        "    t = t.detach().clamp(0,1).cpu().permute(1,2,0).numpy()\n",
        "    return (t*255).astype(np.uint8)\n",
        "\n",
        "def collect_indices(folder):\n",
        "    stems = set()\n",
        "    for ext in EXTS:\n",
        "        for p in glob.glob(os.path.join(folder, f\"*{ext}\")):\n",
        "            name = os.path.basename(p)\n",
        "            m = re.match(r\"(\\d+)\\.[A-Za-z]+$\", name)\n",
        "            if m:\n",
        "                stems.add(int(m.group(1)))\n",
        "    return sorted(stems)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7 numeric images in data/random/low_num: [1, 2, 3, 4, 5, 6, 7]\n",
            "[1] PSNR: 20.97 dB | SSIM: 0.9203\n",
            "[2] PSNR: 22.67 dB | SSIM: 0.9511\n",
            "[3] PSNR: 22.50 dB | SSIM: 0.9585\n",
            "[4] PSNR: 21.85 dB | SSIM: 0.9256\n",
            "[5] PSNR: 23.44 dB | SSIM: 0.9388\n",
            "[6] PSNR: 21.60 dB | SSIM: 0.9238\n",
            "[7] (no GT) visualized only\n",
            "\n",
            "Average over 6 images — PSNR: 22.17 dB | SSIM: 0.9363\n"
          ]
        }
      ],
      "source": [
        "# === Batch enhancement + optional metrics + save triptychs ===\n",
        "indices = collect_indices(LOW_DIR)\n",
        "print(f\"Found {len(indices)} numeric images in {LOW_DIR}: {indices[:10]}{' ...' if len(indices)>10 else ''}\")\n",
        "\n",
        "avg_psnr = 0.0\n",
        "avg_ssim = 0.0\n",
        "n_metric = 0\n",
        "\n",
        "# Keep a lightweight cache for later composite plotting\n",
        "_cache = {}  # idx -> dict(low_pil, enhanced_np, gt_pil or None, metrics or None)\n",
        "\n",
        "for idx in indices:\n",
        "    low_path  = find_file_by_index(LOW_DIR,  idx)\n",
        "    high_path = find_file_by_index(HIGH_DIR, idx)  # May not exist\n",
        "\n",
        "    if not low_path:\n",
        "        continue\n",
        "\n",
        "    low_pil, low_t = load_image_as_tensor(low_path)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        enhanced_t, alphas, betas = model(\n",
        "            low_t,\n",
        "            output_intermediate_images=False,\n",
        "            alpha_scale=ALPHA_SCALE\n",
        "        )\n",
        "\n",
        "    # (Optional) metrics\n",
        "    gt_pil = None\n",
        "    metrics = None\n",
        "    if high_path:\n",
        "        gt_pil, gt_t = load_image_as_tensor(high_path)\n",
        "        metrics = calculate_metrics(enhanced_t, gt_t)\n",
        "        avg_psnr += metrics[\"psnr\"]; avg_ssim += metrics[\"ssim\"]; n_metric += 1\n",
        "        print(f\"[{idx}] PSNR: {metrics['psnr']:.2f} dB | SSIM: {metrics['ssim']:.4f}\")\n",
        "    else:\n",
        "        print(f\"[{idx}] (no GT) visualized only\")\n",
        "\n",
        "    # Save triptych or diptych\n",
        "    enh_np = tensor_to_uint8_img(enhanced_t)\n",
        "    cols = 3 if gt_pil is not None else 2\n",
        "    fig, axes = plt.subplots(1, cols, figsize=(4*cols, 4))\n",
        "    axes = np.atleast_1d(axes)\n",
        "    axes[0].imshow(low_pil);      axes[0].set_title(f\"{idx} - Original\");  axes[0].axis(\"off\")\n",
        "    axes[1].imshow(enh_np);       axes[1].set_title(f\"{idx} - Self-DACE\"); axes[1].axis(\"off\")\n",
        "    if gt_pil is not None:\n",
        "        axes[2].imshow(gt_pil);   axes[2].set_title(f\"{idx} - Ground Truth\");  axes[2].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    out_path = os.path.join(\"outputs\", \"random\", \"triptychs_num\", f\"{idx}_triptych.png\")\n",
        "    plt.savefig(out_path, dpi=220)\n",
        "    plt.close()\n",
        "\n",
        "    _cache[idx] = dict(low_pil=low_pil, enhanced_np=enh_np, gt_pil=gt_pil, metrics=metrics)\n",
        "\n",
        "if n_metric:\n",
        "    print(f\"\\nAverage over {n_metric} images — PSNR: {avg_psnr/n_metric:.2f} dB | SSIM: {avg_ssim/n_metric:.4f}\")\n",
        "else:\n",
        "    print(\"\\nNo GT found in high_num; only visualizations were saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[saved] outputs/random/iterations/1_iters.png\n",
            "[saved] outputs/random/iterations/2_iters.png\n",
            "[saved] outputs/random/iterations/3_iters.png\n",
            "[saved] outputs/random/iterations/4_iters.png\n",
            "[saved] outputs/random/iterations/5_iters.png\n",
            "[saved] outputs/random/iterations/6_iters.png\n",
            "[saved] outputs/random/iterations/7_iters.png\n",
            "[skip] 8 not found in data/random/low_num\n",
            "[skip] 9 not found in data/random/low_num\n",
            "[skip] 10 not found in data/random/low_num\n",
            "[skip] 11 not found in data/random/low_num\n",
            "[skip] 12 not found in data/random/low_num\n"
          ]
        }
      ],
      "source": [
        "# === Iteration visualization (Original → Iter1..7 → Final) ===\n",
        "# Set target indices you want to visualize; leave empty to skip.\n",
        "target_indices = [1,2,3,4,5,6,7]  # e.g., [1, 2, 3]\n",
        "if not target_indices:\n",
        "    print(\"No target indices set for iteration visualization. Set target_indices = [1,2,3] to enable.\")\n",
        "else:\n",
        "    for idx in target_indices:\n",
        "        low_path = find_file_by_index(LOW_DIR, idx)\n",
        "        if not low_path:\n",
        "            print(f\"[skip] {idx} not found in {LOW_DIR}\")\n",
        "            continue\n",
        "\n",
        "        low_pil, low_t = load_image_as_tensor(low_path)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            enhanced_final, alphas, betas, inter_list = model(\n",
        "                low_t,\n",
        "                output_intermediate_images=True,\n",
        "                alpha_scale=ALPHA_SCALE\n",
        "            )\n",
        "\n",
        "        tiles = [np.array(low_pil)]\n",
        "        for step_t in inter_list:\n",
        "            tiles.append(tensor_to_uint8_img(step_t))\n",
        "        tiles.append(tensor_to_uint8_img(enhanced_final))\n",
        "\n",
        "        titles = [\"Original\"] + [f\"Iter {i+1}\" for i in range(len(inter_list))] + [\"Final\"]\n",
        "        n = len(tiles)\n",
        "\n",
        "        fig, axes = plt.subplots(1, n, figsize=(3*n, 3))\n",
        "        for ax, im, tt in zip(axes, tiles, titles):\n",
        "            ax.imshow(im); ax.set_title(tt); ax.axis(\"off\")\n",
        "        plt.tight_layout()\n",
        "        out_path = os.path.join(\"outputs\", \"random\", \"iterations\", f\"{idx}_iters.png\")\n",
        "        plt.savefig(out_path, dpi=220)\n",
        "        plt.close()\n",
        "        print(f\"[saved] {out_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "f71410d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Auto-picked indices with GT: [1, 2, 3, 4, 5, 6]\n",
            "Saved composite figure to: outputs/random/composites/comparison_original_selfdace_gt.png\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/g7/t4hq6htx355_vhxmxcsqvt5c0000gn/T/ipykernel_11412/3169625310.py:71: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
            "  plt.show()\n"
          ]
        }
      ],
      "source": [
        "# === Final composite figure: Original vs Self-DACE vs Ground Truth ===\n",
        "# Choose which indices to include. If empty, we auto-pick up to the first 6 indices that have GT.\n",
        "composite_indices = []  # e.g., [1,2,3]; leave empty to auto-pick\n",
        "\n",
        "# Ensure we have a cache from the batch run\n",
        "try:\n",
        "    _ = _cache\n",
        "except NameError:\n",
        "    _cache = {}\n",
        "\n",
        "# If the cache is empty (e.g., you didn't run the batch cell), rebuild a minimal cache for chosen indices\n",
        "def ensure_cache(indices):\n",
        "    for idx in indices:\n",
        "        if idx in _cache:\n",
        "            continue\n",
        "        low_path  = find_file_by_index(LOW_DIR,  idx)\n",
        "        if not low_path:\n",
        "            continue\n",
        "        high_path = find_file_by_index(HIGH_DIR, idx)\n",
        "\n",
        "        low_pil, low_t = load_image_as_tensor(low_path)\n",
        "        with torch.no_grad():\n",
        "            enhanced_t, alphas, betas = model(\n",
        "                low_t,\n",
        "                output_intermediate_images=False,\n",
        "                alpha_scale=ALPHA_SCALE\n",
        "            )\n",
        "        enh_np = tensor_to_uint8_img(enhanced_t)\n",
        "\n",
        "        gt_pil = None\n",
        "        if high_path:\n",
        "            gt_pil, _ = load_image_as_tensor(high_path)\n",
        "\n",
        "        _cache[idx] = dict(low_pil=low_pil, enhanced_np=enh_np, gt_pil=gt_pil, metrics=None)\n",
        "\n",
        "if not composite_indices:\n",
        "    # Auto-pick indices that have GT\n",
        "    with_gt = [idx for idx, pack in _cache.items() if pack.get(\"gt_pil\") is not None]\n",
        "    if not with_gt:\n",
        "        print(\"No GT images available for composite; please set composite_indices to any indices you want to visualize.\")\n",
        "    else:\n",
        "        composite_indices = sorted(with_gt)[:6]\n",
        "        print(\"Auto-picked indices with GT:\", composite_indices)\n",
        "else:\n",
        "    ensure_cache(composite_indices)\n",
        "\n",
        "if composite_indices:\n",
        "    rows = len(composite_indices)\n",
        "    fig, axes = plt.subplots(rows, 3, figsize=(15, 4*rows))\n",
        "    if rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    for i, idx in enumerate(composite_indices):\n",
        "        pack = _cache.get(idx)\n",
        "        if pack is None:\n",
        "            continue\n",
        "        low_pil   = pack[\"low_pil\"]\n",
        "        enh_np    = pack[\"enhanced_np\"]\n",
        "        gt_pil    = pack[\"gt_pil\"]\n",
        "\n",
        "        axes[i, 0].imshow(low_pil);   axes[i, 0].set_title(f\"{idx} - Original\");    axes[i, 0].axis(\"off\")\n",
        "        axes[i, 1].imshow(enh_np);    axes[i, 1].set_title(f\"{idx} - Self-DACE\");   axes[i, 1].axis(\"off\")\n",
        "        if gt_pil is not None:\n",
        "            axes[i, 2].imshow(gt_pil); axes[i, 2].set_title(f\"{idx} - Ground Truth\"); axes[i, 2].axis(\"off\")\n",
        "        else:\n",
        "            axes[i, 2].axis(\"off\"); axes[i, 2].set_title(f\"{idx} - (No GT)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    out_path = os.path.join(\"outputs\", \"random\", \"composites\", \"comparison_original_selfdace_gt.png\")\n",
        "    plt.savefig(out_path, dpi=220)\n",
        "    plt.show()\n",
        "    print(f\"Saved composite figure to: {out_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "7180",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
